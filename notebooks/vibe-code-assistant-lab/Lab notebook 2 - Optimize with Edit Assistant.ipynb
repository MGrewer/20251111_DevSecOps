{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba16881a-f76c-41e9-88e4-44046d1e24f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Vibe Coding with Databricks Assistant Lab \n",
    "## Notebook 2 - Optimizing Code with the Edit Assistant\n",
    "\n",
    "Welcome to Notebook 2 of the Vibe Coding with Databricks Assistant Lab. In this session, you will explore the edit functionality of the Databricks Assistant. In the scenario a fellow employee has conducted ad-hoc analysis for the demand sensing datasets, however this employee had not taken their Databricks Academy training courses and was not up to speed on the Databricks Assistant. \n",
    "\n",
    "Help our teammate by optimizing their code so that the analysis is running fresh, fast, and error free!\n",
    "\n",
    "This notebook uses serverless compute - please ensure you have are connected to serverless by selecting the Connect drop down and then the Serverless compute option.\n",
    "\n",
    "![[serverless compute]](./includes/2.0_serverless_compute.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc180f52-7eaf-44fc-b0e8-255344d65c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analysis Context\n",
    "This notebook produces insights for three main questions regarding the demand sensing datasets.\n",
    "\n",
    "### August 2025 Grocery Sales by Date\n",
    "**Intent:** Provide a chart of sales by date for August 2025 assuming August just wrapped up and we want to check trends and if there is any late arriving data.\n",
    "\n",
    "**Issues:** The analysis is done in Pandas, a single node engine and does not take avantage of our serverless spark cluster to distribute the work. Pandas was chosen for its built in plotting capabilities, not knowing that Spark 4.0 added native plotting capabilities as well.\n",
    "\n",
    "### Lowest Competitor Price\n",
    "**Intent:** We seek to provide a competitive price across our products compared to a few select competitor grocers. Understand who presents the lowest price per product by day.\n",
    "\n",
    "**Issues:** This implementation uses applyInPandas to sort per group, converts to Python objects, and reconstructs rows which can be slow for large groups.\n",
    "\n",
    "### Total Sales by Region\n",
    "**Intent:** We want to highlight and promote best practices among regional leadership, identify which region is leading in sales.\n",
    "\n",
    "**Issues:** This query uses a highly inneficient cartesian join with Pandas that will not complete. Spark would increase performance and its optimizer defaults to better joins such as sortMerge and Broadcast joins. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "954bc792-7889-47ce-91a2-f65537ef459d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimization Instructions\n",
    "Our goal is to help this analyst optimize their notebook and insights. Open the Databricks Assistant and open a new, clean thread with the plus icon. In the bottom right, select the \"Edit\" mode.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./includes/2.1_assistant_edit.png\" alt=\"Databricks Assistant Edit\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3477c3c-dba8-45d0-a3ba-bbc700e2e2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Enter the following text into the assistant:\n",
    "\n",
    "```\n",
    "Optimize each analysis cell of the notebook. \n",
    "Take into account databricks and spark best practices. \n",
    "Clean up python and pandas code into spark using efficient built in plotting, functions, and joins. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d030823-3f23-4c4a-af15-88b4d62f5bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In edit mode, the Assistant is more careful to plan out its procedure. It can review multiple cells at once which can be helpful when optimizing ETL pipelines, ML training runs, or Analysis which builds off preceding cells. \n",
    "\n",
    "Review the poorly optimized cells below to see suggested code changes leveraging the green/red colors. \n",
    "\n",
    "![Code Suggestions in Edit Mode](./includes/2.3_code_corrections.png)\n",
    "\n",
    "Once you have reviewed the suggestions, select the blue Accept button and run each cell below to see the performance improvements. Notice cell 8: Lowest Competitor Price Analysis goes from 2 minute execution to seconds and cell 9 wouldn't even complete before with its taxing, memory inefficient join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1afa91-a9c7-491b-b9d6-6bf5f5d39cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales = spark.read.table(\"lp_dev.vibe_code_assistant_lab.sales_silver\")\n",
    "df_comp = spark.read.table(\"lp_dev.vibe_code_assistant_lab.competitor_pricing_silver\")\n",
    "df_prod = spark.read.table(\"lp_dev.vibe_code_assistant_lab.products\")\n",
    "df_stores = spark.read.table(\"lp_dev.vibe_code_assistant_lab.stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392217d1-fda0-4af3-9177-5c2082775fca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "August 2025 Grocery Sales by Date"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to pandas\n",
    "pdf_sales = df_sales.toPandas()\n",
    "\n",
    "# Convert 'event_date' column to datetime in pandas\n",
    "pdf_sales['event_date'] = pd.to_datetime(pdf_sales['event_date'])\n",
    "\n",
    "# Filter for August 2025 data only\n",
    "pdf_sales_aug_2025 = pdf_sales[\n",
    "    (pdf_sales['event_date'].dt.year == 2025) & (pdf_sales['event_date'].dt.month == 8)\n",
    "]\n",
    "\n",
    "# Group by event_date and sum total_amount in pandas\n",
    "pdf_sales_by_day = pdf_sales_aug_2025.groupby('event_date')['total_amount'].sum().reset_index()\n",
    "\n",
    "# Display as a line chart using pandas\n",
    "pdf_sales_by_day.plot(kind=\"line\", x=\"event_date\", y=\"total_amount\", title=\"Total Sales by Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2fa32f7-82ca-4d2b-a620-02ee584908e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lowest Competitor Price Analysis"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"product_id\", T.StringType(), True),\n",
    "    T.StructField(\"event_date\", T.DateType(), True),\n",
    "    T.StructField(\"best_competitor\", T.StringType(), True),\n",
    "    T.StructField(\"min_comp_price\", T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "def pick_min_comp(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Convert types and sort every group in Python\n",
    "    pdf[\"price\"] = pdf[\"price\"].astype(float)\n",
    "    pdf = pdf.sort_values([\"price\", \"competitor\"], ascending=[True, True])\n",
    "    top = pdf.iloc[0] if len(pdf) > 0 else None\n",
    "    if top is None:\n",
    "        return pd.DataFrame({\"product_id\": [pdf[\"product_id\"].iloc[0] if len(pdf) > 0 else None],\n",
    "                             \"event_date\": [pdf[\"event_date\"].iloc[0] if len(pdf) > 0 else None],\n",
    "                             \"best_competitor\": [None],\n",
    "                             \"min_comp_price\": [None]})\n",
    "    return pd.DataFrame({\"product_id\": [top[\"product_id\"]],\n",
    "                         \"event_date\": [top[\"event_date\"]],\n",
    "                         \"best_competitor\": [top[\"competitor\"]],\n",
    "                         \"min_comp_price\": [top[\"price\"]]})\n",
    "\n",
    "min_comp_df = (df_comp\n",
    "  .groupBy(\"product_id\", \"event_date\")\n",
    "  .applyInPandas(pick_min_comp, schema=schema)\n",
    ")\n",
    "\n",
    "# Join without hints; materialize intermediate tables unnecessarily\n",
    "min_comp_df.write.mode(\"overwrite\").saveAsTable(\"lp_dev.vibe_code_assistant_lab.tmp_min_comp_bad\")\n",
    "tmp = spark.table(\"lp_dev.vibe_code_assistant_lab.tmp_min_comp_bad\")\n",
    "\n",
    "final = (tmp.join(df_prod, on=\"product_id\", how=\"left\")\n",
    "  .select(\n",
    "    tmp.product_id,\n",
    "    tmp.event_date,\n",
    "    F.when((df_prod.base_price.isNotNull()) & ((tmp.min_comp_price.isNull()) | (df_prod.base_price <= tmp.min_comp_price)), F.lit(\"us\"))\n",
    "     .otherwise(tmp.best_competitor).alias(\"best_vendor\"),\n",
    "    F.when((df_prod.base_price.isNotNull()) & ((tmp.min_comp_price.isNull()) | (df_prod.base_price <= tmp.min_comp_price)), df_prod.base_price)\n",
    "     .otherwise(tmp.min_comp_price).alias(\"best_price\")\n",
    "  )\n",
    ")\n",
    "display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f088fc-151c-4fd7-9659-38c4fef95cce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Total Sales by Store Region"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrames to pandas\n",
    "pdf_sales = df_sales.toPandas()\n",
    "pdf_stores = df_stores.toPandas()\n",
    "\n",
    "# --- Inefficient cartesian join: create a constant key in both frames, merge, then filter ---\n",
    "pdf_cross = (\n",
    "    pdf_sales.assign(_tmp_key=1)\n",
    "    .merge(pdf_stores.assign(_tmp_key=1), on=\"_tmp_key\", suffixes=(\"_sales\", \"_stores\"))\n",
    "    .drop(columns=[\"_tmp_key\"])\n",
    ")\n",
    "\n",
    "# Filter down to the intended join condition AFTER the cross join\n",
    "pdf_joined = (\n",
    "    pdf_cross.loc[pdf_cross[\"store_id_sales\"] == pdf_cross[\"store_id_stores\"], \n",
    "                  [\"store_id_sales\", \"region\", \"total_amount\"]]\n",
    "    .rename(columns={\"store_id_sales\": \"store_id\"})\n",
    ")\n",
    "\n",
    "# Compute total sales by store region (still pandas, single-threaded)\n",
    "pdf_total_sales = (\n",
    "    pdf_joined\n",
    "    .groupby(\"region\", as_index=False)[\"total_amount\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Display as a chart\n",
    "pdf_total_sales.plot(kind=\"bar\", x=\"region\", y=\"total_amount\", legend=False, title=\"Total Sales by Store Region (Bad Cartesian)\")\n",
    "display(pdf_total_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01301691-3641-4a7f-a746-8f2541e6a42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lp_dev.vibe_code_assistant_lab.tmp_min_comp_bad\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Lab notebook 2 - Optimize with Edit Assistant",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
