{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba16881a-f76c-41e9-88e4-44046d1e24f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Vibe Coding with Databricks Assistant Lab \n",
    "## Notebook 2 - Optimizing Code with the Edit Assistant\n",
    "\n",
    "Welcome to Notebook 2 of the Vibe Coding with Databricks Assistant Lab. In this session, you will explore the edit functionality of the Databricks Assistant. In the scenario a fellow employee has conducted ad-hoc analysis for the demand sensing datasets, however this employee had not taken their Databricks Academy training courses and was not up to speed on the Databricks Assistant. \n",
    "\n",
    "Help our teammate by optimizing their code so that the analysis is running fresh, fast, and error free!\n",
    "\n",
    "This notebook uses serverless compute - please ensure you have are connected to serverless by selecting the Connect drop down and then the Serverless compute option.\n",
    "\n",
    "![[serverless compute]](./includes/2.0_serverless_compute.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc180f52-7eaf-44fc-b0e8-255344d65c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analysis Context\n",
    "This notebook produces insights for three main questions regarding the demand sensing datasets.\n",
    "\n",
    "### August 2025 Grocery Sales by Date\n",
    "Intent: Provide a chart of sales by date for August 2025 assuming August just wrapped up and we want to check trends and if there is any late arriving data.\n",
    "\n",
    "Issues: The analysis is done in Pandas, a single node engine and does not take avantage of our serverless spark cluster to distribute the work. Pandas was chosen for its built in plotting capabilities, not knowing that Spark 4.0 added native plotting capabilities as well.\n",
    "\n",
    "### Lowest Competitor Price\n",
    "Intent: We seek to provide a competitive price across our products compared to a few select competitor grocers. Understand who presents the lowest price per product by day.\n",
    "\n",
    "Issues: This implementation uses applyInPandas to sort per group, converts to Python objects, and reconstructs rows which can be slow for large groups.\n",
    "\n",
    "### Total Sales by Region\n",
    "Intent: We want to highlight and promote best practices among regional leadership, identify which region is leading in sales.\n",
    "\n",
    "Issues: This query uses a highly inneficient cartesian join with Pandas that will not complete. Spark would increase performance and its optimizer defaults to better joins such as sortMerge and Broadcast joins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1afa91-a9c7-491b-b9d6-6bf5f5d39cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales = spark.read.table(\"lp_dev.vibe_code_assistant_lab.sales_silver\")\n",
    "df_comp = spark.read.table(\"lp_dev.vibe_code_assistant_lab.competitor_pricing_silver\")\n",
    "df_prod = spark.read.table(\"lp_dev.vibe_code_assistant_lab.products\")\n",
    "df_stores = spark.read.table(\"lp_dev.vibe_code_assistant_lab.stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392217d1-fda0-4af3-9177-5c2082775fca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "August 2025 Grocery Sales by Date"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"aW1wb3J0IHB5c3Bhcmsuc3FsLmZ1bmN0aW9ucyBhcyBGCgojIEZpbHRlciBmb3IgQXVndXN0IDIwMjUgZGF0YSBvbmx5IHVzaW5nIFNwYXJrCnNhbGVzX2F1Z18yMDI1ID0gKAogICAgZGZfc2FsZXMKICAgIC5maWx0ZXIoKEYueWVhcignZXZlbnRfZGF0ZScpID09IDIwMjUpICYgKEYubW9udGgoJ2V2ZW50X2RhdGUnKSA9PSA4KSkKICAgIC5ncm91cEJ5KCdldmVudF9kYXRlJykKICAgIC5hZ2coRi5zdW0oJ3RvdGFsX2Ftb3VudCcpLmFsaWFzKCd0b3RhbF9hbW91bnQnKSkKICAgIC5vcmRlckJ5KCdldmVudF9kYXRlJykKKQoKIyBEaXNwbGF5IGFzIGEgbGluZSBjaGFydCB1c2luZyBTcGFyaydzIGJ1aWx0LWluIHBsb3R0aW5nCmRpc3BsYXkoc2FsZXNfYXVnXzIwMjUp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView00778bb\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView00778bb\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView00778bb\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView00778bb) SELECT `event_date`,SUM(`total_amount`) `column_6fc257bd392` FROM q GROUP BY `event_date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView00778bb\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "event_date",
             "id": "column_6fc257bd391"
            },
            "y": [
             {
              "column": "total_amount",
              "id": "column_6fc257bd392",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_6fc257bd392": {
             "name": "total_amount",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "84e55be0-8404-42a0-ab3d-1d72fa905ab3",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.0625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "event_date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "event_date",
           "type": "column"
          },
          {
           "alias": "column_6fc257bd392",
           "args": [
            {
             "column": "total_amount",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Filter for August 2025 data only using Spark\n",
    "sales_aug_2025 = (\n",
    "    df_sales\n",
    "    .filter((F.year('event_date') == 2025) & (F.month('event_date') == 8))\n",
    "    .groupBy('event_date')\n",
    "    .agg(F.sum('total_amount').alias('total_amount'))\n",
    "    .orderBy('event_date')\n",
    ")\n",
    "\n",
    "# Display as a line chart using Spark's built-in plotting\n",
    "display(sales_aug_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2fa32f7-82ca-4d2b-a620-02ee584908e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lowest Competitor Price Analysis"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"product_id\", T.StringType(), True),\n",
    "    T.StructField(\"event_date\", T.DateType(), True),\n",
    "    T.StructField(\"best_competitor\", T.StringType(), True),\n",
    "    T.StructField(\"min_comp_price\", T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "# Find the lowest competitor price per product and date\n",
    "min_comp_df = (\n",
    "    df_comp\n",
    "    .groupBy('product_id', 'event_date')\n",
    "    .agg(\n",
    "        F.min('price').alias('min_comp_price')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Find the competitor(s) with the minimum price per product and date\n",
    "window = (\n",
    "    Window.partitionBy('product_id', 'event_date').orderBy('price', 'competitor')\n",
    ")\n",
    "min_competitor_df = (\n",
    "    df_comp\n",
    "    .withColumn('rn', F.row_number().over(window))\n",
    "    .filter(F.col('rn') == 1)\n",
    "    .select('product_id', 'event_date', F.col('competitor').alias('best_competitor'), F.col('price').alias('min_comp_price'))\n",
    ")\n",
    "\n",
    "# Join with products to compare with base_price\n",
    "final = (\n",
    "    min_competitor_df.join(df_prod, on='product_id', how='left')\n",
    "    .select(\n",
    "        min_competitor_df.product_id,\n",
    "        min_competitor_df.event_date,\n",
    "        F.when((df_prod.base_price.isNotNull()) & ((min_competitor_df.min_comp_price.isNull()) | (df_prod.base_price <= min_competitor_df.min_comp_price)), F.lit('us'))\n",
    "         .otherwise(min_competitor_df.best_competitor).alias('best_vendor'),\n",
    "        F.when((df_prod.base_price.isNotNull()) & ((min_competitor_df.min_comp_price.isNull()) | (df_prod.base_price <= min_competitor_df.min_comp_price)), df_prod.base_price)\n",
    "         .otherwise(min_competitor_df.min_comp_price).alias('best_price')\n",
    "    )\n",
    ")\n",
    "display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f088fc-151c-4fd7-9659-38c4fef95cce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Total Sales by Store Region"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Join sales and stores on store_id\n",
    "joined = df_sales.join(df_stores, on='store_id', how='inner')\n",
    "\n",
    "# Compute total sales by region\n",
    "total_sales_by_region = (\n",
    "    joined.groupBy('region')\n",
    "    .agg(F.sum('total_amount').alias('total_amount'))\n",
    "    .orderBy(F.desc('total_amount'))\n",
    ")\n",
    "\n",
    "# Display as a bar chart\n",
    "display(total_sales_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01301691-3641-4a7f-a746-8f2541e6a42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lp_dev.vibe_code_assistant_lab.tmp_min_comp_bad\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Lab notebook 2 [Solution] - Optimize with Edit Assistant",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
